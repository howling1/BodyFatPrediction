{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import open3d as o3d\n",
    "import os\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDataset(InMemoryDataset):\n",
    "    def __init__(self, raw_data_root, dataset_root, basic_features_path, ids_root, target_name, gender, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        Creates the InMemoryDataset object, convenient structure for graph training.\n",
    "        :param raw_data_root: path for the input mesh data \n",
    "        :param dataset_root: path where the in memory dataset will be written\n",
    "        :param basic_features_path: path of the \"basic_features.csv\"\n",
    "        :param ids_root: path of the file saving the ids of selected samples\n",
    "        :param target_name: name of the feature to be predicted\n",
    "            [\"sex\",\"bmi\",\"age\",\"weight\",\"height\"]\n",
    "        :param gender: 0 for female, 1 for male\n",
    "        \"\"\"\n",
    "        self.ids = np.load(ids_root)\n",
    "        self.raw_data_root = raw_data_root\n",
    "        self.basic_features_path = basic_features_path\n",
    "        self.target_name = target_name\n",
    "\n",
    "        if not os.path.exists(dataset_root):\n",
    "            os.makedirs(dataset_root)\n",
    "        super(IMDataset, self).__init__(root = dataset_root, transform = transform, pre_transform = pre_transform)\n",
    "        if gender == 0: #female\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        elif gender == 1: #male\n",
    "            self.data, self.slices = torch.load(self.processed_paths[1])\n",
    " \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        file_names = (pd.Series(self.ids).astype(str) + \".ply\").values.tolist()\n",
    "        return [os.path.join(str(Path(self.raw_data_root)), file_name) for file_name in file_names]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        processed_root = self.root + \"/\" + self.target_name\n",
    "\n",
    "        if not os.path.exists(processed_root):\n",
    "            os.makedirs(processed_root)\n",
    "\n",
    "        return [processed_root + \"/female_dataset.pt\", processed_root + \"/male_dataset.pt\"]\n",
    " \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Combines the vertex coordinates, edge connectivity indexes and chosen target feature.\n",
    "        \"\"\"\n",
    "        male_data_list = []\n",
    "        female_data_list = []\n",
    "\n",
    "        if self.target_name == \"vat\":\n",
    "            cols = [\"22407-2.0\"]\n",
    "        elif self.target_name == \"asat\":\n",
    "            cols = [\"22408-2.0\"]\n",
    "        elif self.target_name == \"all\":\n",
    "            cols = [\"22407-2.0\", \"22408-2.0\"]\n",
    "\n",
    "        features = pd.read_csv(self.basic_features_path, usecols =[\"eid\", \"31-0.0\"] + cols)\n",
    "\n",
    "        for file_path in tqdm(self.raw_file_names):\n",
    "            _id = int(os.path.splitext(os.path.basename(file_path))[0])\n",
    "            _sex = int(features[features['eid']==_id][\"31-0.0\"].values[0]) #female: 0, male: 1\n",
    "            _y = torch.tensor(features[features['eid']==_id][cols].values[0]).float()\n",
    "            if torch.isnan(_y).any().item():\n",
    "                continue\n",
    "            \n",
    "            _mesh = o3d.io.read_triangle_mesh(file_path)\n",
    "            _vertices = torch.from_numpy(np.asarray(_mesh.vertices)).float()\n",
    "            _triangles = _mesh.triangles\n",
    "\n",
    "            \"\"\"\n",
    "            necessary to get the edge connectivity since the original data has only face connectivity\n",
    "            we need [0,1],[1,2],[0,2],[1,0],[2,1],[2,0] instead of [0,1,2]\n",
    "            including reverse as well to make the graph bi-directional\n",
    "            \"\"\"\n",
    "            edge_list = []\n",
    "            for t in _triangles:\n",
    "                edge_list = edge_list + [[t[0], t[1]], [t[1], t[0]], [t[0], t[2]], [t[2], t[0]], [t[1], t[2]], [t[2], t[1]]]\n",
    "\n",
    "            _edges = torch.from_numpy(np.unique(np.array(edge_list), axis=0)).long().permute(1, 0)\n",
    "\n",
    "            if _sex == 0:\n",
    "                female_data_list.append(Data(x=_vertices, edge_index=_edges, y=_y))\n",
    "            else:\n",
    "                male_data_list.append(Data(x=_vertices, edge_index=_edges, y=_y))\n",
    "        \n",
    "        data_female, slices_female = self.collate(female_data_list)\n",
    "        data_male, slices_male = self.collate(male_data_list)\n",
    "        #saves sex based datasets\n",
    "        torch.save((data_female, slices_female), self.processed_paths[0])\n",
    "        torch.save((data_male, slices_male), self.processed_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTERED_ROOT = \"/vol/space/projects/ukbb/projects/silhouette/registered500_faces\" # the path of the dir saving the .ply registered data\n",
    "INMEMORY_ROOT = '/vol/space/projects/ukbb/projects/silhouette/imdataset/registered500_faces_imdataset' # the root dir path to save all the artifacts ralated of the InMemoryDataset\n",
    "FEATURES_PATH = \"/vol/space/projects/ukbb/projects/silhouette/ukb668815_imaging.csv\"   \n",
    "IDS_PATH = \"/vol/space/projects/ukbb/projects/silhouette/eids_filtered.npy\"\n",
    "TARGET = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_female = IMDataset(REGISTERED_ROOT, INMEMORY_ROOT, FEATURES_PATH, IDS_PATH, TARGET, 0)\n",
    "dataset_male = IMDataset(REGISTERED_ROOT, INMEMORY_ROOT, FEATURES_PATH, IDS_PATH, TARGET, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([261, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset_male)[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01558336615562439\n",
      "0.01558336615562439\n"
     ]
    }
   ],
   "source": [
    "print(dataset_female.data.x.element_size() * dataset_female.data.x.numel()/(1024 * 1024 * 1024))\n",
    "dataset_female.data.x = dataset_female.data.x.float()\n",
    "print(dataset_female.data.x.element_size() * dataset_female.data.x.numel()/(1024 * 1024 * 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00019502639770507812\n",
      "9.751319885253906e-05\n"
     ]
    }
   ],
   "source": [
    "print(dataset_female.data.y.element_size() * dataset_female.data.y.numel()/(1024 * 1024 * 1024))\n",
    "dataset_female.data.y = dataset_female.data.y.float()\n",
    "print(dataset_female.data.y.element_size() * dataset_female.data.y.numel()/(1024 * 1024 * 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8972172141075134\n",
      "1.4486086070537567\n"
     ]
    }
   ],
   "source": [
    "print(dataset_female.data.edge_index.element_size() * dataset_female.data.edge_index.numel()/(1024 * 1024 * 1024))\n",
    "dataset_female.data.edge_index = dataset_female.data.edge_index.int()\n",
    "print(dataset_female.data.edge_index.element_size() * dataset_female.data.edge_index.numel()/(1024 * 1024 * 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.583462119102478\n",
      "0.1458655297756195\n"
     ]
    }
   ],
   "source": [
    "print(dataset_female.data.edge_index.element_size() * dataset_female.data.edge_index.numel()/(1024 * 1024 * 1024))\n",
    "dataset_female.data.edge_index = dataset_female.data.edge_index.to(torch.int16)\n",
    "print(dataset_female.data.edge_index.element_size() * dataset_female.data.edge_index.numel()/(1024 * 1024 * 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr_siyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
