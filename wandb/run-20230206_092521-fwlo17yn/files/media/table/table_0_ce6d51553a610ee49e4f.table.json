{"columns": ["params"], "data": [["{'experiment_name': 'age_prediction_5k', 'batch_size': 32, 'epochs': 2000, 'learning_rate': 0.005, 'weight_decay': 0.01, 'task': 'regression', 'print_every_n': 200, 'validate_every_n': 200}"], ["{'gnn_conv': <class 'torch_geometric.nn.conv.sage_conv.SAGEConv'>, 'in_features': 3, 'num_hiddens': 4, 'num_layers': 3, 'encoder_channels': [], 'decoder_channels': [8], 'num_classes': 1, 'jk_mode': 'cat', 'aggregation': 'max', 'apply_dropedge': True, 'apply_bn': True, 'apply_dropout': True}"], ["JKNet(\n  (conv_layers): ModuleList(\n    (0): SAGEConv(3, 4, aggr=mean)\n    (1): SAGEConv(4, 4, aggr=mean)\n    (2): SAGEConv(4, 4, aggr=mean)\n  )\n  (bn_layers): ModuleList(\n    (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (jk): JumpingKnowledge(cat)\n  (linear_encoder): Sequential(\n    (0): Identity()\n  )\n  (final_projection): Sequential(\n    (0): Linear(in_features=12, out_features=8, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=8, out_features=1, bias=True)\n    (4): Identity()\n  )\n)"]]}